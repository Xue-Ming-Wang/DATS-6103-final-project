# %%[markdown]

# This file uses the cleaned LDA filing data as an input, iterating through records to create a dataframe with dummy variables for each category.

# %%[markdown]
### Setting up data
# %%
# Importing libraries

# Importing json to handle raw file in repository + API calls
import json

# Importing requests for API calls
import requests

# Importing itertools to combine nested lists into one list conveniently
import itertools

# Importing pandas for dataframe creation/transformation
import pandas as pd

# Importing matplotlib and seaborn for EDA data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing scikitlearn for modeling and feature selection work
import sklearn


# %%
# Load data from raw file in repository

with open("cleaned_LDA_filings_v2.json") as file:

    data = json.load(file)

# %%
# Make API calls for static data on issue names and entity codes/names

# Part 1: Issues; create a URL, make the call, and add each full name to issuesList

issuesURL = r'https://lda.senate.gov/api/v1/constants/filing/lobbyingactivityissues/'

issuesRequest = requests.get(issuesURL)

issuesJson = issuesRequest.json()

issuesList = []

for issueDict in issuesJson:
    issuesList.append(issueDict['name'])

# Part 2: Entities; create a URL, make the call, and add each full name + code to their own respective lists

entitiesURL = 'https://lda.senate.gov/api/v1/constants/filing/governmententities/'
entitiesRequest = requests.get(entitiesURL)

entitiesJson = entitiesRequest.json()

entitiesIdList = []
entitiesNameList = []

for entityDict in entitiesJson:
    entitiesIdList.append(entityDict['id'])
    entitiesNameList.append(entityDict['name'])

# %%
# Loop through nested records to generate a flattened list for our dataframe

# Initialize a dataList variable to append record-specific results to 
dataList = []

# For each record:
for record in data:
    # Define an empty list to fill out (will ultimately be appended to datalist)
    recordList = []

    # Append a field for income or expenses, depending on which one is available (the nature of filings means that one or the other will be present but not both)
    if record['income'] != None:
        recordList.append(float(record['income']))
    elif record['expenses'] != None:
        recordList.append(float(record['expenses']))
    else:
        recordList.append(0)

    # If the type was expenses, mark True for internal lobbying
    if record['expenses'] != None:
        recordList.append(1)
    else:
        recordList.append(0)

    # Initialize an empty list for government entities
    recordEntitiesList = []
    
    # Initialize an empty list for lobbyists
    recordLobbyistsList = []

    # For each issue that could show up:
    for issue in issuesList:
        # Create/switch issueCheck to False
        issueCheck = False

        # For each activity in the lobbying activities list:
        for activity in record['lobbying_activities']:
            # If this activity matches the issue in question:
            if activity['general_issue_code_display'] == issue:
                # Change issueCheck to True
                issueCheck = True

            # Add any entitity IDs found to our entities list
            recordEntitiesList.append(activity['government_entities'])

            # Add any lobbyists found to our lobbyists list
            recordLobbyistsList.append(activity['lobbyists'])

        # If the issue was found among the record's activities, append True to the recordList, otherwise append False
        if issueCheck == True:
            recordList.append(1)
        else:
            recordList.append(0)

    # Convert the recordEntitiesList, which is now filled with nested entitity lists from all activity disclosures, to a 1-D list
    recordEntitiesConcat = list(itertools.chain.from_iterable(recordEntitiesList))

    # Conver the recordLobbyistsList, which is now filled with nested lobbyist lists from all activity disclosures, to a 1-D list
    recordLobbyistsConcat = list(itertools.chain.from_iterable(recordLobbyistsList))

    # Initialize a list for unique entities
    recordEntitiesUnique = []

    # For each "raw" entity, check to see if it is already present in the unique list before including it
    for recordEntityRaw in recordEntitiesConcat:
        if recordEntityRaw not in recordEntitiesUnique:
            recordEntitiesUnique.append(recordEntityRaw)
    
    # For each entity ID in the list gathered earlier:
    for entityId in entitiesIdList:
        # Create/switch entityCheck to False
        entityCheck = False

        # For each unique entity ID in this record:
        for recordEntityId in recordEntitiesUnique:
            # If this ID matches the value present in the entity ID list:
            if entityId == recordEntityId:
                # Change entityCheck to True
                entityCheck = True
        
        # If the enitity was found among the record's activities, append True to the recordList, otherwise append False
        if entityCheck == True:
            recordList.append(1)
        else:
            recordList.append(0)
    
    # Initialize a list for unique lobbyists
    recordLobbyistsUnique = []

    # For each "raw" lobbyist, check to see if they are already present in the unique list before including them
    for recordLobbyistRaw in recordLobbyistsConcat:
        if recordLobbyistRaw not in recordLobbyistsUnique:
            recordLobbyistsUnique.append(recordLobbyistRaw)
    
    # Append a count for all unique lobbyists
    recordList.append(len(recordLobbyistsUnique))

    # Initialize a list for new lobbyists only
    recordLobbyistsNew = []
    # For each lobbyist in the unique lobbyist list:
    for recordLobbyistRaw in recordLobbyistsUnique:
        # If they are new, add them to the new lobbyist list
        if recordLobbyistRaw['new'] == True:
            recordLobbyistsNew.append(recordLobbyistRaw['new'])
    
    # Append a count for all new, unique lobbyists
    recordList.append(len(recordLobbyistsNew))


    # Add the full recordList to the dataList
    dataList.append(recordList)
# %%

# Define column names

# Initialize an empty list for column names
colNameList = []

# Append column name values in the order that they were added earlier
colNameList.append(['income or expenses', 'internal lobbying'])
colNameList.append(issuesList)
colNameList.append(entitiesNameList)
colNameList.append(['lobbyist_count_all', 'lobbyist_count_new'])

# Convert to a 1-D list
colNameList = list(itertools.chain.from_iterable(colNameList))

# %%
# Convert to dataframe

df = pd.DataFrame(dataList, columns = colNameList)

print(df)

# %%
# Droping two columns
df = df.drop(columns=['income or expenses', 'internal lobbying'])

# Policy areas
policy_areas= df.drop(columns=['lobbyist_count_new', 'lobbyist_count_all'])

# Lobby efforts
lobbyist= ['lobbyist_count_new', 'lobbyist_count_all']

# %%[markdown]
### Apriori algorithm - simple version
# %%

# Implement Apriori algorithm for association rule mining
from mlxtend.frequent_patterns import apriori, association_rules

# Convert policy areas to binary indicators
policy_areas_binary = pd.get_dummies(policy_areas)

# Generate frequent itemsets using Apriori algorithm
frequent_itemsets = apriori(policy_areas, min_support=0.1, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Display the association rules
print("Association Rules:")
print(rules)

# %%[markdown]
### Apriori algorithm - Defense
# %%
# Import necessary libraries
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules


# Convert policy areas to binary indicators
policy_areas_binary = df.drop(columns=['lobbyist_count_new', 'lobbyist_count_all'])

# Apply Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(policy_areas_binary, min_support=0.1, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

# Filter rules related to the policy area of interest
policy_area_of_interest = "Defense"
relevant_rules = rules[rules['antecedents'].apply(lambda x: policy_area_of_interest in x)]

# Display the relevant association rules
print("Association Rules for Policy Area:", policy_area_of_interest)
print(relevant_rules)

# %%

df.columns.tolist()

# %%[markdown]
### Apriori algorithm within policy area  - complicated version
# %%
from mlxtend.frequent_patterns import apriori, association_rules

# Convert policy_areas to binary values (1 if lobbying effort in that policy area, 0 otherwise)
policy_areas_binary = policy_areas.applymap(lambda x: 1 if x > 0 else 0)

# Find frequent itemsets using the Apriori algorithm
frequent_itemsets = apriori(policy_areas_binary, min_support=0.1, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Sort the rules by lift (a measure of how much more often the antecedent and consequent occur together than we would expect if they were statistically independent)
rules = rules.sort_values(by='lift', ascending=False)

# Filter out rules where the antecedent and consequent are the same policy area
rules = rules[rules['antecedents'] != rules['consequents']]

# Display the resulting association rules
print("Frequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules:")
print(rules)

# %%[markdown]
### Apriori algorithm - policy area vs lobby efforts
# %%

from mlxtend.frequent_patterns import apriori, association_rules

# Convert policy_areas to binary values (1 if lobbying effort in that policy area, 0 otherwise)
policy_areas_binary = policy_areas.applymap(lambda x: 1 if x > 0 else 0)

# Find frequent itemsets using the Apriori algorithm
frequent_itemsets = apriori(policy_areas_binary, min_support=0.1, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

# Sort the rules by lift 
rules = rules.sort_values(by='lift', ascending=False)

# Filter out rules where the antecedent and consequent are the same policy area
rules = rules[rules['antecedents'] != rules['consequents']]

# Rename the consequents column to 'lobbyist_count_new' and 'lobbyist_count_all'
rules['consequents'] = rules['consequents'].apply(lambda x: frozenset({'lobbyist_count_new', 'lobbyist_count_all'}) - x)

# Display the resulting association rules
print("Frequent Itemsets:")
print(frequent_itemsets)
print("\nAssociation Rules:")
print(rules)


# %%[markdown]
### K-mean Clustering
# %%
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Feature scaling
scaler = StandardScaler()
scaled_policy_areas = scaler.fit_transform(policy_areas)

# Initialize KMeans clustering with desired number of clusters
kmeans = KMeans(n_clusters=5, random_state=42)  

# Fit KMeans to the scaled data
kmeans.fit(scaled_policy_areas)

# Assign cluster labels to each policy area
policy_areas['cluster'] = kmeans.labels_

# Analyze the clusters
cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)  

# Print cluster centers
print("Cluster Centers:")
for i, center in enumerate(cluster_centers):
    print(f"Cluster {i+1}: {center}")

# Analyze the distribution of policy areas within each cluster
cluster_distribution = policy_areas.groupby('cluster').mean()

# Print cluster distribution
print("\nCluster Distribution:")
print(cluster_distribution)


# %%[markdown]
### K-mean Clustering fewer dataset
# %%
# Round the cluster centers to 2 decimal places
cluster_centers_rounded = [[round(value, 2) for value in center] for center in cluster_centers]

# Print cluster centers
print("Cluster Centers:")
for i, center in enumerate(cluster_centers_rounded):
    print(f"Cluster {i+1}: {center}")

# Filter out columns with low values for easier interpretation
significant_columns = cluster_distribution.columns[cluster_distribution.max() > 0.5]

# Analyze the distribution of policy areas within each cluster using only significant columns
cluster_distribution_filtered = cluster_distribution[significant_columns]

# Print cluster distribution with significant columns
print("\nCluster Distribution:")
print(cluster_distribution_filtered)

# %%[markdown]
### K-mean Clustering <0.3
# %%
# Round the cluster centers to 2 decimal places
cluster_centers_rounded = [[round(value, 2) for value in center] for center in cluster_centers]

# Print cluster centers
print("Cluster Centers:")
for i, center in enumerate(cluster_centers_rounded):
    print(f"Cluster {i+1}: {center}")

# Filter out columns with low values for easier interpretation
significant_columns = cluster_distribution.columns[(cluster_distribution.max() > 0.3) & (cluster_distribution.max() < 1)]

# Analyze the distribution of policy areas within each cluster using only significant columns
cluster_distribution_filtered = cluster_distribution[significant_columns]

# Print cluster distribution with significant columns
print("\nCluster Distribution (only significant columns above 0.3):")
print(cluster_distribution_filtered)



# %%
# Plot the cluster distribution

plt.figure(figsize=(10, 6))
sns.heatmap(cluster_distribution_filtered, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title('Cluster Distribution of Significant Policy Areas')
plt.xlabel('Policy Areas')
plt.ylabel('Cluster')
plt.xticks(rotation=45, ha='right')
plt.show()

# %%[markdown]
### K-mean Clustering - elbow method
# %%
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Scale the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(policy_areas)

# Determine optimal number of clusters using the elbow method
wcss = []
for i in range(1, 20):  
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

# Plot the elbow curve
plt.plot(range(1, 20), wcss)  
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()
# %%
# Apply K-means clustering with optimal number of clusters
optimal_clusters = 17  
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Analyze results
# Assign cluster labels back to DataFrame
policy_areas['Cluster'] = clusters

# Calculate mean lobbying efforts for each cluster
cluster_means = policy_areas.groupby('Cluster').mean()

# Visualize cluster means
plt.figure(figsize=(10, 6))
sns.heatmap(cluster_means, annot=True, cmap="YlGnBu")
plt.title('Mean Lobbying Efforts for Each Cluster')
plt.xlabel('Policy Areas')
plt.ylabel('Cluster')
plt.xticks(rotation=45, ha='right')
plt.show()

# %%
# %%[markdown]
### Train a classification model -Random Forest 
# %%
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

X = policy_areas.values  # Input features

# Perform K-means clustering with 17 clusters
kmeans = KMeans(n_clusters=17, random_state=42)
cluster_assignments = kmeans.fit_predict(X)

y = df['lobbyist_count_new']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Interpret feature importances
feature_importances = rf_classifier.feature_importances_
feature_importance_dict = dict(zip(policy_areas.columns, feature_importances))
sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# Print only the top 20 feature importances
print("\nTop 20 Feature Importances:")
for feature, importance in sorted_feature_importance[:20]:
    print(f"{feature}: {importance}")

# %%
# Extract feature importances from the trained Random Forest classifier
rf_feature_importances = rf_classifier.feature_importances_
rf_feature_importance_dict = dict(zip(policy_areas.columns, rf_feature_importances))
sorted_rf_feature_importance = sorted(rf_feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

# Plotting the top 20 features for Random Forest
plt.figure(figsize=(10, 6))
plt.barh(range(10), [val[1] for val in sorted_rf_feature_importance[:10]], align='center')
plt.yticks(range(10), [val[0] for val in sorted_rf_feature_importance[:10]])
plt.xlabel('Feature Importance')
plt.title('Top 10 Feature Importances - Random Forest')
plt.gca().invert_yaxis()
plt.show()

# %%
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier()

# Perform cross-validation
cv_scores = cross_val_score(rf_classifier, X, y, cv=5) 

# Print cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())

# Plot the cross-validation scores
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='skyblue')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores - Random Forest Classifier')
plt.xticks(range(1, len(cv_scores) + 1))
plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label='Mean CV Score')
plt.legend()
plt.show()
# %%[markdown]
### Train a classification model - logistic regression

# %%
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the input features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the logistic regression classifier with a higher max_iter value
logistic_reg = LogisticRegression(max_iter=1000) 

# Train the classifier on the scaled training data
logistic_reg.fit(X_train_scaled, y_train)

# Make predictions on the scaled test data
y_pred = logistic_reg.predict(X_test_scaled)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))
# %%
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# Create a pipeline with scaling and logistic regression
pipeline = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))

# Perform cross-validation
cv_scores = cross_val_score(pipeline, X, y, cv=5)  # You can adjust the number of folds (cv) as needed

# Print cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())
# %%
# Plot the cross-validation scores
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='lightgreen')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores - Logistic Regression')
plt.xticks(range(1, len(cv_scores) + 1))
plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label='Mean CV Score')
plt.legend()
plt.show()

# %%[markdown]
### Train a classification model - decision tree

# %%
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the decision tree classifier
decision_tree = DecisionTreeClassifier()

# Train the classifier on the training data
decision_tree.fit(X_train, y_train)

# Make predictions on the test data
y_pred = decision_tree.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

# %%
# Extract feature importances from the trained Decision Tree classifier
dt_feature_importances = decision_tree.feature_importances_
dt_feature_importance_dict = dict(zip(policy_areas.columns, dt_feature_importances))
sorted_dt_feature_importance = sorted(dt_feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

plt.figure(figsize=(10, 6))
plt.barh(range(10), [val[1] for val in sorted_dt_feature_importance[:10]], align='center')
plt.yticks(range(10), [val[0] for val in sorted_dt_feature_importance[:10]])
plt.xlabel('Feature Importance')
plt.title('Top 10 Feature Importances - Decision Tree')
plt.gca().invert_yaxis()
plt.show()

# %%
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

# Initialize the Decision Tree classifier
decision_tree = DecisionTreeClassifier()

# Perform cross-validation
cv_scores = cross_val_score(decision_tree, X, y, cv=5) 

# Print cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Score:", cv_scores.mean())

# Plot the cross-validation scores
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(cv_scores) + 1), cv_scores, color='lightcoral')
plt.xlabel('Fold')
plt.ylabel('Accuracy')
plt.title('Cross-Validation Scores - Decision Tree Classifier')
plt.xticks(range(1, len(cv_scores) + 1))
plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label='Mean CV Score')
plt.legend()
plt.show()

# %%