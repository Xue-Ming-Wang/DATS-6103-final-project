# %%[markdown]

# This file uses the cleaned LDA filing data as an input, iterating through records to create a dataframe with dummy variables for each category.

# %%[markdown]
### Setting up data
# %%
# Importing libraries

# Importing json to handle raw file in repository + API calls
import json

# Importing requests for API calls
import requests

# Importing itertools to combine nested lists into one list conveniently
import itertools

# Importing pandas for dataframe creation/transformation
import pandas as pd

# Importing matplotlib and seaborn for EDA data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing scikitlearn for modeling and feature selection work
import sklearn


# %%
# Load data from raw file in repository

with open("cleaned_LDA_filings_v2.json") as file:

    data = json.load(file)

# %%
# Make API calls for static data on issue names and entity codes/names

# Part 1: Issues; create a URL, make the call, and add each full name to issuesList

issuesURL = r'https://lda.senate.gov/api/v1/constants/filing/lobbyingactivityissues/'

issuesRequest = requests.get(issuesURL)

issuesJson = issuesRequest.json()

issuesList = []

for issueDict in issuesJson:
    issuesList.append(issueDict['name'])

# Part 2: Entities; create a URL, make the call, and add each full name + code to their own respective lists

entitiesURL = 'https://lda.senate.gov/api/v1/constants/filing/governmententities/'
entitiesRequest = requests.get(entitiesURL)

entitiesJson = entitiesRequest.json()

entitiesIdList = []
entitiesNameList = []

for entityDict in entitiesJson:
    entitiesIdList.append(entityDict['id'])
    entitiesNameList.append(entityDict['name'])

# %%
# Loop through nested records to generate a flattened list for our dataframe

# Initialize a dataList variable to append record-specific results to 
dataList = []

# For each record:
for record in data:
    # Define an empty list to fill out (will ultimately be appended to datalist)
    recordList = []

    # Append a field for income or expenses, depending on which one is available (the nature of filings means that one or the other will be present but not both)
    if record['income'] != None:
        recordList.append(float(record['income']))
    elif record['expenses'] != None:
        recordList.append(float(record['expenses']))
    else:
        recordList.append(0)

    # If the type was expenses, mark True for internal lobbying
    if record['expenses'] != None:
        recordList.append(1)
    else:
        recordList.append(0)

    # Initialize an empty list for government entities
    recordEntitiesList = []
    
    # Initialize an empty list for lobbyists
    recordLobbyistsList = []

    # For each issue that could show up:
    for issue in issuesList:
        # Create/switch issueCheck to False
        issueCheck = False

        # For each activity in the lobbying activities list:
        for activity in record['lobbying_activities']:
            # If this activity matches the issue in question:
            if activity['general_issue_code_display'] == issue:
                # Change issueCheck to True
                issueCheck = True

            # Add any entitity IDs found to our entities list
            recordEntitiesList.append(activity['government_entities'])

            # Add any lobbyists found to our lobbyists list
            recordLobbyistsList.append(activity['lobbyists'])

        # If the issue was found among the record's activities, append True to the recordList, otherwise append False
        if issueCheck == True:
            recordList.append(1)
        else:
            recordList.append(0)

    # Convert the recordEntitiesList, which is now filled with nested entitity lists from all activity disclosures, to a 1-D list
    recordEntitiesConcat = list(itertools.chain.from_iterable(recordEntitiesList))

    # Conver the recordLobbyistsList, which is now filled with nested lobbyist lists from all activity disclosures, to a 1-D list
    recordLobbyistsConcat = list(itertools.chain.from_iterable(recordLobbyistsList))

    # Initialize a list for unique entities
    recordEntitiesUnique = []

    # For each "raw" entity, check to see if it is already present in the unique list before including it
    for recordEntityRaw in recordEntitiesConcat:
        if recordEntityRaw not in recordEntitiesUnique:
            recordEntitiesUnique.append(recordEntityRaw)
    
    # For each entity ID in the list gathered earlier:
    for entityId in entitiesIdList:
        # Create/switch entityCheck to False
        entityCheck = False

        # For each unique entity ID in this record:
        for recordEntityId in recordEntitiesUnique:
            # If this ID matches the value present in the entity ID list:
            if entityId == recordEntityId:
                # Change entityCheck to True
                entityCheck = True
        
        # If the enitity was found among the record's activities, append True to the recordList, otherwise append False
        if entityCheck == True:
            recordList.append(1)
        else:
            recordList.append(0)
    
    # Initialize a list for unique lobbyists
    recordLobbyistsUnique = []

    # For each "raw" lobbyist, check to see if they are already present in the unique list before including them
    for recordLobbyistRaw in recordLobbyistsConcat:
        if recordLobbyistRaw not in recordLobbyistsUnique:
            recordLobbyistsUnique.append(recordLobbyistRaw)
    
    # Append a count for all unique lobbyists
    recordList.append(len(recordLobbyistsUnique))

    # Initialize a list for new lobbyists only
    recordLobbyistsNew = []
    # For each lobbyist in the unique lobbyist list:
    for recordLobbyistRaw in recordLobbyistsUnique:
        # If they are new, add them to the new lobbyist list
        if recordLobbyistRaw['new'] == True:
            recordLobbyistsNew.append(recordLobbyistRaw['new'])
    
    # Append a count for all new, unique lobbyists
    recordList.append(len(recordLobbyistsNew))


    # Add the full recordList to the dataList
    dataList.append(recordList)
# %%

# Define column names

# Initialize an empty list for column names
colNameList = []

# Append column name values in the order that they were added earlier
colNameList.append(['income or expenses', 'internal lobbying'])
colNameList.append(issuesList)
colNameList.append(entitiesNameList)
colNameList.append(['lobbyist_count_all', 'lobbyist_count_new'])

# Convert to a 1-D list
colNameList = list(itertools.chain.from_iterable(colNameList))

# %%
# Convert to dataframe

df = pd.DataFrame(dataList, columns = colNameList)

print(df)

# %%
# Droping two columns
df = df.drop(columns=['income or expenses', 'internal lobbying'])

# Policy areas
policy_areas= df.drop(columns=['lobbyist_count_new', 'lobbyist_count_all'])

# Lobby efforts
lobbyist= ['lobbyist_count_new', 'lobbyist_count_all']
# %%[markdown]
### EDA
# %%[markdown]
### correlation matrix
# %%
# Calculate the correlation matrix
correlation_matrix = policy_areas.corr()

# Display the correlation matrix
print(correlation_matrix)


# %%[markdown]
### Top/lowest correlation matrix
# %%
## Sort the correlation matrix
sorted_corr = correlation_matrix.unstack().sort_values(ascending=False)

# Remove correlations of variables with themselves (which will be 1)
sorted_corr = sorted_corr[sorted_corr != 1]

# Print out the 10 highest correlations
print("Top 10 highest correlations:")
for i, (indices, correlation) in enumerate(sorted_corr.head(10).items(), 1):
    index1, index2 = indices
    print(f"{i}. {index1} - {index2}: {correlation}")

# Print out the 10 lowest correlations
filtered_corr = sorted_corr[sorted_corr > 0.001].tail(10)

# Print out the 10 lowest correlations
print("\nTop 10 lowest correlations:")
for i, (indices, correlation) in enumerate(filtered_corr.items(), 1):
    index1, index2 = indices
    print(f"{i}. {index1} - {index2}: {correlation}")

# %%
    
# Identify the closest associations using correlation matrix
correlation_matrix = policy_areas.corr()

# Sort the correlations
sorted_corr = correlation_matrix.unstack().sort_values(ascending=False)

# Remove correlations of variables with themselves (which will be 1)
sorted_corr = sorted_corr[sorted_corr != 1]

# Display the top correlated pairs
print("Top correlated policy areas:")
print(sorted_corr.head(10))

# %%
    
# %%[markdown]
### correlation map
# %%
# Visualize the correlation matrix using a heatmap
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Policy Areas')
plt.show()




# %%[markdown]
### chi-square

# %%[markdown]
### chi-square - test with fewer areas
# %%
'''
from scipy.stats import chi2_contingency

# List of policy areas for comparison
policy_areas = ["Defense", "Education", "Accounting", "Aerospace"]

# Initialize dictionaries to store results
chi2_values = {}
p_values = {}

# Iterate over pairs of policy areas
for i in range(len(policy_areas)):
    for j in range(i+1, len(policy_areas)):
        policy_area_1 = policy_areas[i]
        policy_area_2 = policy_areas[j]
        
        # Create a contingency table
        contingency_table = pd.crosstab(df[policy_area_1], df[policy_area_2])
        
        # Perform the Chi-square Test
        chi2, p_value, _, _ = chi2_contingency(contingency_table)
        
        # Store results
        chi2_values[(policy_area_1, policy_area_2)] = chi2
        p_values[(policy_area_1, policy_area_2)] = p_value

# Display the results
for (area1, area2), chi2 in chi2_values.items():
    print(f"Chi-square value for {area1} and {area2}: {chi2}")
    
for (area1, area2), p_value in p_values.items():
    print(f"P-value for {area1} and {area2}: {p_value}")



'''

# %%
# %%[markdown]
### chi-square few
# %%

from scipy.stats import chi2_contingency
import itertools

# Initialize dictionaries to store results
chi2_values = {}
p_values = {}

# Iterate over pairs of policy areas
for area1, area2 in itertools.combinations(policy_areas, 2):
    # Create a contingency table
    contingency_table = pd.crosstab(df[area1], df[area2])
    
    # Perform the Chi-square Test
    chi2, p_value, _, _ = chi2_contingency(contingency_table)
    
    # Store results
    chi2_values[(area1, area2)] = chi2
    p_values[(area1, area2)] = p_value

# Display the results
for (area1, area2), chi2 in chi2_values.items():
    print(f"Chi-square value for {area1} and {area2}: {chi2}")
    
for (area1, area2), p_value in p_values.items():
    print(f"P-value for {area1} and {area2}: {p_value}")

# %%
# Filter pairs with p-value < 0.05
significant_pairs = [(area1, area2) for (area1, area2), p_value in p_values.items() if p_value < 0.05]

# Display the significant pairs
print("\nSignificant pairs with p-value < 0.05:")
for area1, area2 in significant_pairs:
    print(f"{area1} and {area2} are significant. P-value: {p_values[(area1, area2)]}")
# %%
# Count significant pairs
num_significant_pairs = len(significant_pairs)

# Display the count
print("Number of significant pairs with p-value < 0.05:", num_significant_pairs)

# %%
